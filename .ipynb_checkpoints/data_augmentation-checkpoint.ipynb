{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os, sys, datetime, time, random, fnmatch, math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import skimage.metrics\n",
    "\n",
    "import datasets, custom_transforms\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "main_file_directory = \"D:/data/JSRT/HQ_JSRT_and_BSE-JSRT/train\"\n",
    "file_sets = [\"normal\",\"suppressed\"]\n",
    "sample_keys_images = [\"source\", \"boneless\"]\n",
    "total_epochs = 20 # 200 images in the original training dataset\n",
    "\n",
    "composed_transforms = transforms.Compose([\n",
    "    #custom_transforms.HistogramEqualisation(sample_keys_images),\n",
    "    custom_transforms.RandomAutocontrast(sample_keys_images, cutoff_limits=(0.05,0.05)),\n",
    "    custom_transforms.Resize(sample_keys_images,256),\n",
    "    custom_transforms.CenterCrop(sample_keys_images,256),\n",
    "    custom_transforms.RandomAffine(sample_keys_images, degrees=10,translate=(0.1,0.1),scale=(0.9,1.1)),\n",
    "    custom_transforms.ToTensor(sample_keys_images),\n",
    "    custom_transforms.ImageComplement(sample_keys_images),\n",
    "                                    ])\n",
    "\n",
    "ds = datasets.JSRT_CXR(os.path.join(main_file_directory,file_sets[0]), \n",
    "                       os.path.join(main_file_directory, file_sets[1]), \n",
    "                       transform=composed_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_images = Path('./train_augmented_noEqualise')\n",
    "path_to_save_images.mkdir(parents=True, exist_ok=True)\n",
    "counter=0\n",
    "for epoch in range(total_epochs):\n",
    "    for count, sample in enumerate(ds):\n",
    "        try:\n",
    "            savename = sample[\"Patient\"]+\"-\"+str(counter)+\".png\"\n",
    "        except:\n",
    "            savename = str(counter)+\".png\"\n",
    "        vutils.save_image( sample[\"source\"], os.path.join(path_to_save_images, \"normal\", savename))\n",
    "        vutils.save_image( sample[\"boneless\"], os.path.join(path_to_save_images, \"suppressed\", savename))\n",
    "        counter+=1\n",
    "        \n",
    "    # For each epoch, visual \n",
    "    fig,ax=plt.subplots(1,2,figsize=(10,5))\n",
    "    ax[0].imshow(np.squeeze(sample[\"source\"].numpy()),cmap=\"gray\")\n",
    "    ax[1].imshow(np.squeeze(sample[\"boneless\"].numpy()),cmap=\"gray\")\n",
    "    plt.show()\n",
    "print(\"Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
