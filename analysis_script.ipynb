{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a565ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset chosen is: QEH_onePerPatient\n",
      "524\n",
      "torch.Size([10, 1, 256, 256])\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os, sys, datetime, time, random, fnmatch, math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import skimage.metrics\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms as tvtransforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.utils as vutils\n",
    "import torch.utils.tensorboard as tensorboard\n",
    "import torch.nn as nn\n",
    "\n",
    "import datasets, custom_transforms, RajaramanModel, pytorch_msssim\n",
    "\n",
    "# Flags:\n",
    "flag_saveReportPictures = True # set to True to produce a batch of example images for writing reports and papers\n",
    "flag_saveSuppressedImages = True # set to True to produce images for further AI training\n",
    "\n",
    "# Paths\n",
    "PATH_SAVE_NETWORK_INTERMEDIATE = \"./runs/Rajaraman_ResNet/v2_HQ_noEqualised/network_final.tar\"\n",
    "key_source = \"source\" # this is the dictionary key for the original radiograph in the datasets\n",
    "key_boneless = \"boneless\" # this is the dictionary key for the bone-suppressed radiograph in the datasets file\n",
    "# Data\n",
    "_batch_size = 10\n",
    "image_spatial_size = (256,256)\n",
    "\n",
    "switch = \"QEH_onePerPatient\" #\"internal_test\" # #\"Dongrong_Test\" #\n",
    "save_description = \"_v2_HQ_notEqualised_inputsNotEqualised\"\n",
    "\n",
    "print(\"The dataset chosen is: \" + switch)\n",
    "def transform_dataset(keys_images):\n",
    "    transforms = tvtransforms.Compose([\n",
    "                                 #custom_transforms.HistogramEqualisation(keys_images),\n",
    "                                 custom_transforms.Resize(keys_images, image_spatial_size),\n",
    "                                 custom_transforms.ToTensor(keys_images),\n",
    "                                 ])\n",
    "    return transforms\n",
    "if switch == \"JSRT_all\":\n",
    "    directory_source = \"D:/data/JSRT/JSRT/\"\n",
    "    directory_boneless = \"D:/data/JSRT/BSE_JSRT/\"\n",
    "    keys_images = [key_source, key_boneless]\n",
    "    ds = datasets.JSRT_CXR(directory_source, directory_boneless, \n",
    "                           transform=transform_dataset(keys_images))\n",
    "elif switch == \"internal_test\":\n",
    "    directory_source = \"D:/data/JSRT/HQ_JSRT_and_BSE-JSRT/test/normal/\"\n",
    "    directory_boneless = \"D:/data/JSRT/HQ_JSRT_and_BSE-JSRT/test/suppressed/\"\n",
    "    keys_images = [key_source, key_boneless]\n",
    "    ds = datasets.JSRT_CXR(directory_source, directory_boneless, \n",
    "                           transform=transform_dataset(keys_images))\n",
    "elif switch == \"JSRT_NN\":\n",
    "    directory_source = \"D:/data/JSRT/JSRT_NN/\"\n",
    "    directory_boneless = None\n",
    "    keys_images = [key_source]\n",
    "    ds = datasets.JSRT_CXR(directory_source, directory_boneless, \n",
    "                           transform=transform_dataset(keys_images))\n",
    "    \n",
    "elif switch == \"QEH_full\":\n",
    "    externalTest_directory = r\"D:/data/QEH_COVID19_DATASET/CXR_pngs/\"\n",
    "    keys_images = [key_source]\n",
    "    ds = datasets.POLYU_COVID19_CXR_CT_Cohort1(externalTest_directory,\n",
    "                                 transform=transform_dataset(keys_images))\n",
    "elif switch == \"QEH_onePerPatient\":\n",
    "    externalTest_directory = r\"D:/data/QEH_COVID19_DATASET/QEH_Earliest_CXR_per_patient/\"\n",
    "    keys_images = [key_source]\n",
    "    ds = datasets.POLYU_COVID19_CXR_CT_Cohort1(externalTest_directory,\n",
    "                                 transform=transform_dataset(keys_images))\n",
    "elif switch ==\"Dongrong_Test\":\n",
    "    normal_path = \"D:/data/DongrongDataSets/test/NORMAL\"\n",
    "    pneumonia_path = \"D:/data/DongrongDataSets/test/PNEUMONIA/\"\n",
    "    covid_path = \"D:/data/DongrongDataSets/test/COVID/\"\n",
    "    keys_images = [key_source]\n",
    "    ds = datasets.DongrongCOVIDDataset(normal_path, pneumonia_path, covid_path, \n",
    "                                       transform=transform_dataset(keys_images))\n",
    "    \n",
    "else:\n",
    "    raise RuntimeError(\"Dataset unknown.  Please input the details in the datasets.py file\")\n",
    "print(len(ds))\n",
    "dl = DataLoader(ds, _batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Select test \n",
    "sample = next(iter(dl))\n",
    "print(sample[key_source].shape)\n",
    "\n",
    "## Code for putting things on the GPU\n",
    "ngpu = 1 #torch.cuda.device_count()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print(device)\n",
    "if (torch.cuda.is_available()):\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "569fa13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint './runs/Rajaraman_ResNet/v2_HQ_noEqualised/network_final.tar'\n",
      "=> loaded checkpoint './runs/Rajaraman_ResNet/v2_HQ_noEqualised/network_final.tar' (epoch 200, reals shown 800000)\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Network\n",
    "input_array_size = (_batch_size, 1, image_spatial_size[0], image_spatial_size[1])\n",
    "net = RajaramanModel.ResNet_BS(input_array_size)\n",
    "#net = nn.DataParallel(net, list(range(ngpu)))\n",
    "if os.path.isfile(PATH_SAVE_NETWORK_INTERMEDIATE):\n",
    "    print(\"=> loading checkpoint '{}'\".format(PATH_SAVE_NETWORK_INTERMEDIATE))\n",
    "    checkpoint = torch.load(PATH_SAVE_NETWORK_INTERMEDIATE, map_location='cpu')\n",
    "    start_epoch = checkpoint['epoch_next']\n",
    "    reals_shown_now = checkpoint['reals_shown']\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"=> loaded checkpoint '{}' (epoch {}, reals shown {})\".format(PATH_SAVE_NETWORK_INTERMEDIATE, \n",
    "                                                                        start_epoch, reals_shown_now))\n",
    "else:\n",
    "    print(\"=> NO CHECKPOINT FOUND AT '{}'\" .format(PATH_SAVE_NETWORK_INTERMEDIATE))\n",
    "    raise RuntimeError(\"No checkpoint found at specified path.\")\n",
    "\n",
    "net = net.to(device)\n",
    "# Set to testing mode\n",
    "net.eval()\n",
    "print(\".\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d77793b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving a set of bone suppressed images\n",
      "bone_suppressed\\QEH_onePerPatient_v2_HQ_notEqualised_inputsNotEqualised\n",
      "Batch Number:0\n",
      "Batch Number:1\n",
      "Batch Number:2\n",
      "Batch Number:3\n",
      "Batch Number:4\n",
      "Batch Number:5\n",
      "Batch Number:6\n",
      "Batch Number:7\n",
      "Batch Number:8\n",
      "Batch Number:9\n",
      "Batch Number:10\n",
      "Batch Number:11\n",
      "Batch Number:12\n",
      "Batch Number:13\n",
      "Batch Number:14\n",
      "Batch Number:15\n",
      "Batch Number:16\n",
      "Batch Number:17\n",
      "Batch Number:18\n",
      "Batch Number:19\n",
      "Batch Number:20\n",
      "Batch Number:21\n",
      "Batch Number:22\n",
      "Batch Number:23\n",
      "Batch Number:24\n",
      "Batch Number:25\n",
      "Batch Number:26\n",
      "Batch Number:27\n",
      "Batch Number:28\n",
      "Batch Number:29\n",
      "Batch Number:30\n",
      "Batch Number:31\n",
      "Batch Number:32\n",
      "Batch Number:33\n",
      "Batch Number:34\n"
     ]
    }
   ],
   "source": [
    "if flag_saveSuppressedImages:\n",
    "    print(\"Saving a set of bone suppressed images\")\n",
    "    path_to_save_images = Path(os.path.join(\"bone_suppressed\",switch+save_description))\n",
    "    path_to_save_images.mkdir(parents=True, exist_ok=True)\n",
    "    print(path_to_save_images)\n",
    "    iters=0\n",
    "    for ii, data in enumerate(dl):\n",
    "        with torch.no_grad():\n",
    "            input_data = data[key_source].to(device)\n",
    "            out = net(input_data)\n",
    "            print(\"Batch Number:\" + str(ii))\n",
    "            out = out.cpu()\n",
    "        \n",
    "        for iii, image in enumerate(out):\n",
    "            try:\n",
    "                savename = data[\"Patient\"][iii]+\".png\"\n",
    "            except:\n",
    "                savename = str(iters)+\".png\"\n",
    "            vutils.save_image( image, os.path.join(path_to_save_images, savename))\n",
    "            iters+=1\n",
    "    print(\"Complete saving suppressed images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ec3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display\n",
    "save_directory = os.path.split(PATH_SAVE_NETWORK_INTERMEDIATE)[0]\n",
    "print(save_directory)\n",
    "\n",
    "out = net(sample[key_source])\n",
    "out = out.detach()\n",
    "for batch_idx in range(_batch_size):\n",
    "    if \"boneless\" in keys_images:\n",
    "        plt.figure(1)\n",
    "        fig, ax = plt.subplots(1,3, figsize=(15,5))\n",
    "        ax[0].imshow(sample[\"source\"][batch_idx,0,:],cmap='gray')\n",
    "        ax[0].set_title(\"Source\")\n",
    "        ax[0].axis(\"off\")\n",
    "        ax[1].imshow(out[batch_idx,0,:],cmap='gray')\n",
    "        ax[1].set_title(\"Suppressed\")\n",
    "        ax[1].axis(\"off\")\n",
    "        ax[2].imshow(sample[\"boneless\"][batch_idx,0,:],cmap='gray')\n",
    "        ax[2].set_title(\"Ideal\")\n",
    "        ax[2].axis(\"off\")\n",
    "    else:\n",
    "        plt.figure(1)\n",
    "        fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "        ax[0].imshow(sample[\"source\"][batch_idx,0,:],cmap='gray')\n",
    "        ax[0].set_title(\"Source\")\n",
    "        ax[0].axis(\"off\")\n",
    "        ax[1].imshow(out[batch_idx,0,:],cmap='gray')\n",
    "        ax[1].set_title(\"Suppressed\")\n",
    "        ax[1].axis(\"off\")\n",
    "    if flag_saveReportPictures:\n",
    "        plt.savefig(os.path.join(save_directory, switch + \"_comparisonImages_\"+ str(batch_idx) +\".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9bfb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "def PSNR(image, reference, max_reference=1.):\n",
    "    \"\"\" \n",
    "    Peak Signal-to-Noise Ratio\n",
    "    Input image and reference assumed to be Torch Tensors of shape [NxCxHxW]\n",
    "    \"\"\"\n",
    "    # Assume the image \n",
    "    H = image.size(-2)\n",
    "    W = image.size(-1)\n",
    "    MSE = (1/(H*W))*torch.sum((torch.abs(image - reference))**2, (-1,-2))\n",
    "    rtMSE = torch.sqrt(MSE)\n",
    "    \n",
    "    output = 20*torch.log10(max_reference/rtMSE)\n",
    "    return np.atleast_1d(output.numpy().squeeze())\n",
    "\n",
    "def NPS():\n",
    "    \"\"\"Noise Power Spectrum\"\"\"\n",
    "    \n",
    "\n",
    "def SSIM(image, reference):\n",
    "    iters = 0\n",
    "    out_list = np.ndarray(image.size(0))\n",
    "    for im in image:\n",
    "        im = im.numpy()\n",
    "        im = np.moveaxis(im, 0,-1)\n",
    "        ref = reference[iters,:].numpy()\n",
    "        ref = np.moveaxis(ref, 0,-1)\n",
    "        out = skimage.metrics.structural_similarity(im, ref, multichannel=True)\n",
    "        out_list[iters]=out\n",
    "        iters+=1\n",
    "    return np.atleast_1d(out_list)\n",
    "\n",
    "def RMSE(image, reference):\n",
    "    # Assume the image \n",
    "    H = image.size(-2)\n",
    "    W = image.size(-1)\n",
    "    MSE = (1/(H*W))*torch.sum((torch.abs(image - reference))**2, (-1,-2))\n",
    "    RMSE = torch.sqrt(MSE)\n",
    "    out_list = np.squeeze(RMSE.numpy())\n",
    "    return np.atleast_1d(out_list)\n",
    "\n",
    "psnr_dict={\"source_to_boneless\":[], \"suppressed_to_boneless\":[]}\n",
    "ssim_dict={\"source_to_boneless\":[], \"suppressed_to_boneless\":[]}\n",
    "RMSE_dict ={\"source_to_boneless\":[], \"suppressed_to_boneless\":[]}\n",
    "if \"boneless\" in keys_images:\n",
    "    for sample in dl:\n",
    "        # Neural network\n",
    "        out = net(sample[\"source\"])\n",
    "        out = out.detach()\n",
    "        \n",
    "        psnr_dict[\"source_to_boneless\"].append(PSNR(sample[\"source\"], sample[\"boneless\"]))\n",
    "        psnr_dict[\"suppressed_to_boneless\"].append(PSNR(out, sample[\"boneless\"]))\n",
    "        ssim_dict[\"source_to_boneless\"].append(SSIM(sample[\"source\"], sample[\"boneless\"]))\n",
    "        ssim_dict[\"suppressed_to_boneless\"].append(SSIM(out, sample[\"boneless\"]))\n",
    "        RMSE_dict[\"source_to_boneless\"].append(RMSE(sample[\"source\"], sample[\"boneless\"]))\n",
    "        RMSE_dict[\"suppressed_to_boneless\"].append(RMSE(out, sample[\"boneless\"]))\n",
    "#print(\"PSNR original: {} ; after denoising: {} \".format(PSNR(sample[\"source\"], sample[\"boneless\"]).mean(), PSNR(out, sample[\"boneless\"]).mean()))\n",
    "#print(\"SSIM original: {} ; after denoising: {} \".format(SSIM(sample[\"source\"], sample[\"boneless\"]).mean(), SSIM(out, sample[\"boneless\"]).mean()))\n",
    "print(os.path.split(PATH_SAVE_NETWORK_INTERMEDIATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaaa76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall averaging\n",
    "\n",
    "print(\"PSNR Source to Boneless: \" + str(np.concatenate(psnr_dict[\"source_to_boneless\"]).mean()) + \"+/-\" + str(np.std(np.concatenate(psnr_dict[\"source_to_boneless\"]))))\n",
    "print(\"PSNR Suppressed to Boneless: \" + str(np.concatenate(psnr_dict[\"suppressed_to_boneless\"]).mean()) + \"+/-\" + str(np.std(np.concatenate(psnr_dict[\"suppressed_to_boneless\"]))))\n",
    "print(\"SSIM Source to Boneless: \" + str(np.concatenate(ssim_dict[\"source_to_boneless\"]).mean()) + \"+/-\" + str(np.std(np.concatenate(ssim_dict[\"source_to_boneless\"]))))\n",
    "print(\"SSIM Suppressed to Boneless: \" + str(np.concatenate(ssim_dict[\"suppressed_to_boneless\"]).mean()) + \"+/-\" + str(np.std(np.concatenate(ssim_dict[\"suppressed_to_boneless\"]))))\n",
    "print(\"RMSE Source to Boneless: \"+str(np.concatenate(RMSE_dict[\"source_to_boneless\"]).mean()) +\"+/-\"+ str(np.std(np.concatenate(RMSE_dict[\"source_to_boneless\"])))) \n",
    "print(\"RMSE Suppressed to Boneless: \"+str(np.concatenate(RMSE_dict[\"suppressed_to_boneless\"]).mean())+\"+/-\"+str(np.std(np.concatenate(RMSE_dict[\"suppressed_to_boneless\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ed6e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paired averaging\n",
    "def paired_t_test(array1, array2):\n",
    "    array1 = np.concatenate(array1)\n",
    "    array2 = np.concatenate(array2)\n",
    "    \n",
    "    x_diff = array1 - array2\n",
    "    s_diff = np.std(x_diff)\n",
    "    x_diff_mean = np.mean(x_diff)\n",
    "    s_x = s_diff/np.sqrt(len(x_diff))\n",
    "    \n",
    "    t_score = np.abs(x_diff_mean/s_x)\n",
    "    return t_score\n",
    "\n",
    "T_PSNR = paired_t_test(psnr_dict[\"source_to_boneless\"], psnr_dict[\"suppressed_to_boneless\"])\n",
    "T_SSIM = paired_t_test(ssim_dict[\"source_to_boneless\"], ssim_dict[\"suppressed_to_boneless\"])\n",
    "T_RMSE = paired_t_test(RMSE_dict[\"source_to_boneless\"], RMSE_dict[\"suppressed_to_boneless\"])\n",
    "\n",
    "print(\"T-score PSNR: {}\".format(T_PSNR))\n",
    "print(\"T-score SSIM: {}\".format(T_SSIM))\n",
    "print(\"T-score RMSE: {}\".format(T_RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc8310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(path_to_save_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
