{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e962e9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "## Training Script\n",
    "# Run this to train a bone suppression network ResNet-BS, introduced by Rajaraman et al. 2021.\n",
    "# \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os, sys, datetime, time, random, fnmatch, math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import skimage.metrics\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms as tvtransforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, ConcatDataset\n",
    "import torchvision.utils as vutils\n",
    "import torch.utils.tensorboard as tensorboard\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import datasets, custom_transforms, RajaramanModel, pytorch_msssim\n",
    "\n",
    "flag_debug = False\n",
    "flag_load_previous_save = False\n",
    "\n",
    "# Input Directories\n",
    "#data_BSE = \"D:/data/JSRT/augmented/train/target/\"\n",
    "#data_normal = \"D:/data/JSRT/augmented/train/source/\"\n",
    "data_BSE = Path('G:/DanielLam/JSRT/HQ_JSRT_and_BSE-JSRT/177-20-20 split/trainValidate_LowRes/suppressed/')\n",
    "data_normal = Path('G:/DanielLam/JSRT/HQ_JSRT_and_BSE-JSRT/177-20-20 split/trainValidate_LowRes/normal/')\n",
    "#data_val_normal = 'G:/DanielLam/JSRT/HQ_JSRT_and_BSE-JSRT/177-20-20 split/validate/normal'\n",
    "#data_val_BSE = 'G:/DanielLam/JSRT/HQ_JSRT_and_BSE-JSRT/177-20-20 split/validate/suppressed/'\n",
    "\n",
    "# Save directories:\n",
    "output_save_directory = Path(\"./runs/Rajaraman_ResNet/177-20-20/10KFold_4000perEpoch\")\n",
    "output_save_directory.mkdir(parents=True, exist_ok=True)\n",
    "PATH_SAVE_NETWORK_INTERMEDIATE = os.path.join(output_save_directory, 'network_intermediate_{}.tar' )\n",
    "PATH_SAVE_NETWORK_FINAL = os.path.join(output_save_directory, 'network_final_{}.pt')\n",
    "\n",
    "# Image Size:\n",
    "image_spatial_size = (480, 480)\n",
    "_batch_size = 2\n",
    "split_k_folds=10\n",
    "sample_keys_images = [\"source\", \"boneless\"]\n",
    "\n",
    "# Optimisation\n",
    "lr_ini = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "\n",
    "# Training\n",
    "total_num_epochs_paper = 200\n",
    "epoch_factor = 22.8764\n",
    "total_num_epochs = int(epoch_factor*total_num_epochs_paper)\n",
    "\n",
    "# Weight Initialisation\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        nn.init.normal_(m.weight.data, 0., 0.02)\n",
    "        #nn.init.kaiming_normal_(m.weight.data,0)\n",
    "        try:\n",
    "            nn.init.constant_(m.bias.data, 0.)\n",
    "        except:\n",
    "            pass\n",
    "    if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.InstanceNorm2d):\n",
    "        if m.affine:\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias.data, 0.)\n",
    "\n",
    "## Code for putting things on the GPU\n",
    "ngpu = 1 #torch.cuda.device_count()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print(device)\n",
    "if (torch.cuda.is_available()):\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "    if ngpu ==1:\n",
    "        device=torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e173907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current date:\n",
    "current_date=datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Data Loader\n",
    "original_key = \"source\"\n",
    "target_key = \"boneless\"\n",
    "discriminator_keys_images = [original_key, target_key]\n",
    "\n",
    "#ds_training = datasets.JSRT_CXR(data_normal, data_BSE,\n",
    "#                         transform=tvtransforms.Compose([\n",
    "#                             # custom_transforms.HistogramEqualisation(discriminator_keys_images),-- check if training data is already equalised\n",
    "#                             custom_transforms.Resize(discriminator_keys_images, image_spatial_size),\n",
    "#                             custom_transforms.ToTensor(discriminator_keys_images),\n",
    "#                             ])\n",
    "#                      )\n",
    "#ds_val = datasets.JSRT_CXR(data_val_normal, data_val_BSE,\n",
    "#                         transform=tvtransforms.Compose([\n",
    "#                             #custom_transforms.HistogramEqualisation(discriminator_keys_images),\n",
    "#                             custom_transforms.Resize(discriminator_keys_images, image_spatial_size),\n",
    "#                             custom_transforms.ToTensor(discriminator_keys_images),\n",
    "#                             ])\n",
    "#                      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bf0b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rajaraman Loss\n",
    "def criterion_Rajaraman(testImage, referenceImage, alpha=0.84):\n",
    "    \"\"\"\n",
    "    Generates a loss over a mini-batch of images.\n",
    "    Outputs scalars.\n",
    "    Rajaraman, S., Zamzmi, G., Folio, L., Alderson, P., & Antani, S. (2021). Chest X-Ray Bone Suppression for Improving Classification of Tuberculosis-Consistent Findings. In Diagnostics (Vol. 11, Issue 5). https://doi.org/10.3390/diagnostics11050840\n",
    "    \"\"\"\n",
    "    mae = nn.L1Loss() # L2 used for easier optimisation c.f. L1\n",
    "    mae_loss = mae(testImage, referenceImage)\n",
    "    msssim = pytorch_msssim.MSSSIM(window_size=11, size_average=True, channel=1, normalize='relu')\n",
    "    msssim_loss = 1 - msssim(testImage, referenceImage)\n",
    "    total_loss = (1-alpha)*mae_loss + alpha*msssim_loss\n",
    "    return total_loss, mae_loss, msssim_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a01323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boneless\n",
      "Fold 1\n",
      "FLAG: NO CHECKPOINT LOADED.\n",
      "0.001\n",
      "[0/4575][0/89]\tTotal Loss: 0.7398\tMAELoss: 0.3184\tMSSSIM Loss: 0.8201\n",
      "[0/4575][1/89]\tTotal Loss: 0.6066\tMAELoss: 0.2979\tMSSSIM Loss: 0.6654\n",
      "[0/4575][2/89]\tTotal Loss: 0.5838\tMAELoss: 0.2955\tMSSSIM Loss: 0.6387\n",
      "[0/4575][3/89]\tTotal Loss: 0.4591\tMAELoss: 0.2504\tMSSSIM Loss: 0.4989\n",
      "[0/4575][4/89]\tTotal Loss: 0.2803\tMAELoss: 0.2146\tMSSSIM Loss: 0.2928\n",
      "[0/4575][5/89]\tTotal Loss: 0.3430\tMAELoss: 0.3673\tMSSSIM Loss: 0.3383\n",
      "[0/4575][6/89]\tTotal Loss: 0.2927\tMAELoss: 0.3878\tMSSSIM Loss: 0.2746\n",
      "[0/4575][7/89]\tTotal Loss: 0.2081\tMAELoss: 0.3374\tMSSSIM Loss: 0.1834\n",
      "[0/4575][8/89]\tTotal Loss: 0.2266\tMAELoss: 0.1683\tMSSSIM Loss: 0.2376\n",
      "[0/4575][9/89]\tTotal Loss: 0.2486\tMAELoss: 0.1938\tMSSSIM Loss: 0.2590\n",
      "[0/4575][10/89]\tTotal Loss: 0.2459\tMAELoss: 0.1688\tMSSSIM Loss: 0.2605\n",
      "[0/4575][11/89]\tTotal Loss: 0.1661\tMAELoss: 0.1495\tMSSSIM Loss: 0.1692\n",
      "[0/4575][12/89]\tTotal Loss: 0.1821\tMAELoss: 0.0972\tMSSSIM Loss: 0.1983\n",
      "[0/4575][13/89]\tTotal Loss: 0.1458\tMAELoss: 0.0869\tMSSSIM Loss: 0.1570\n",
      "[0/4575][14/89]\tTotal Loss: 0.1250\tMAELoss: 0.0950\tMSSSIM Loss: 0.1307\n",
      "[0/4575][15/89]\tTotal Loss: 0.1395\tMAELoss: 0.0861\tMSSSIM Loss: 0.1496\n",
      "[0/4575][16/89]\tTotal Loss: 0.0891\tMAELoss: 0.0702\tMSSSIM Loss: 0.0927\n",
      "[0/4575][17/89]\tTotal Loss: 0.0962\tMAELoss: 0.1329\tMSSSIM Loss: 0.0892\n",
      "[0/4575][18/89]\tTotal Loss: 0.0625\tMAELoss: 0.0973\tMSSSIM Loss: 0.0559\n",
      "[0/4575][19/89]\tTotal Loss: 0.0751\tMAELoss: 0.1026\tMSSSIM Loss: 0.0699\n",
      "[0/4575][20/89]\tTotal Loss: 0.0589\tMAELoss: 0.0808\tMSSSIM Loss: 0.0547\n",
      "[0/4575][21/89]\tTotal Loss: 0.0532\tMAELoss: 0.0950\tMSSSIM Loss: 0.0453\n",
      "[0/4575][22/89]\tTotal Loss: 0.1083\tMAELoss: 0.1211\tMSSSIM Loss: 0.1058\n",
      "[0/4575][23/89]\tTotal Loss: 0.1087\tMAELoss: 0.1295\tMSSSIM Loss: 0.1047\n",
      "[0/4575][24/89]\tTotal Loss: 0.0794\tMAELoss: 0.0842\tMSSSIM Loss: 0.0785\n",
      "[0/4575][25/89]\tTotal Loss: 0.0558\tMAELoss: 0.0790\tMSSSIM Loss: 0.0513\n",
      "[0/4575][26/89]\tTotal Loss: 0.0686\tMAELoss: 0.0685\tMSSSIM Loss: 0.0686\n",
      "[0/4575][27/89]\tTotal Loss: 0.0563\tMAELoss: 0.0637\tMSSSIM Loss: 0.0549\n",
      "[0/4575][28/89]\tTotal Loss: 0.0567\tMAELoss: 0.0723\tMSSSIM Loss: 0.0537\n",
      "[0/4575][29/89]\tTotal Loss: 0.0664\tMAELoss: 0.0910\tMSSSIM Loss: 0.0617\n",
      "[0/4575][30/89]\tTotal Loss: 0.0482\tMAELoss: 0.0780\tMSSSIM Loss: 0.0426\n",
      "[0/4575][31/89]\tTotal Loss: 0.0456\tMAELoss: 0.0602\tMSSSIM Loss: 0.0428\n",
      "[0/4575][32/89]\tTotal Loss: 0.0611\tMAELoss: 0.0501\tMSSSIM Loss: 0.0632\n",
      "[0/4575][33/89]\tTotal Loss: 0.0527\tMAELoss: 0.0501\tMSSSIM Loss: 0.0533\n",
      "[0/4575][34/89]\tTotal Loss: 0.0626\tMAELoss: 0.0589\tMSSSIM Loss: 0.0633\n",
      "[0/4575][35/89]\tTotal Loss: 0.0502\tMAELoss: 0.0513\tMSSSIM Loss: 0.0500\n",
      "[0/4575][36/89]\tTotal Loss: 0.0336\tMAELoss: 0.0405\tMSSSIM Loss: 0.0323\n",
      "[0/4575][37/89]\tTotal Loss: 0.0361\tMAELoss: 0.0438\tMSSSIM Loss: 0.0346\n",
      "[0/4575][38/89]\tTotal Loss: 0.0509\tMAELoss: 0.0610\tMSSSIM Loss: 0.0490\n",
      "[0/4575][39/89]\tTotal Loss: 0.0473\tMAELoss: 0.0644\tMSSSIM Loss: 0.0441\n",
      "[0/4575][40/89]\tTotal Loss: 0.0382\tMAELoss: 0.0543\tMSSSIM Loss: 0.0351\n",
      "[0/4575][41/89]\tTotal Loss: 0.0452\tMAELoss: 0.0669\tMSSSIM Loss: 0.0410\n",
      "[0/4575][42/89]\tTotal Loss: 0.0387\tMAELoss: 0.0429\tMSSSIM Loss: 0.0379\n",
      "[0/4575][43/89]\tTotal Loss: 0.0448\tMAELoss: 0.0484\tMSSSIM Loss: 0.0441\n",
      "[0/4575][44/89]\tTotal Loss: 0.0372\tMAELoss: 0.0462\tMSSSIM Loss: 0.0355\n",
      "[0/4575][45/89]\tTotal Loss: 0.0324\tMAELoss: 0.0305\tMSSSIM Loss: 0.0328\n",
      "[0/4575][46/89]\tTotal Loss: 0.0388\tMAELoss: 0.0372\tMSSSIM Loss: 0.0392\n",
      "[0/4575][47/89]\tTotal Loss: 0.0486\tMAELoss: 0.0404\tMSSSIM Loss: 0.0501\n",
      "[0/4575][48/89]\tTotal Loss: 0.0536\tMAELoss: 0.0659\tMSSSIM Loss: 0.0512\n",
      "[0/4575][49/89]\tTotal Loss: 0.0406\tMAELoss: 0.0890\tMSSSIM Loss: 0.0313\n",
      "[0/4575][50/89]\tTotal Loss: 0.0479\tMAELoss: 0.0826\tMSSSIM Loss: 0.0413\n",
      "[0/4575][51/89]\tTotal Loss: 0.0417\tMAELoss: 0.0647\tMSSSIM Loss: 0.0373\n",
      "[0/4575][52/89]\tTotal Loss: 0.0361\tMAELoss: 0.0468\tMSSSIM Loss: 0.0340\n",
      "[0/4575][53/89]\tTotal Loss: 0.0356\tMAELoss: 0.0368\tMSSSIM Loss: 0.0353\n",
      "[0/4575][54/89]\tTotal Loss: 0.0442\tMAELoss: 0.0380\tMSSSIM Loss: 0.0454\n",
      "[0/4575][55/89]\tTotal Loss: 0.0415\tMAELoss: 0.0349\tMSSSIM Loss: 0.0428\n",
      "[0/4575][56/89]\tTotal Loss: 0.0480\tMAELoss: 0.0386\tMSSSIM Loss: 0.0498\n",
      "[0/4575][57/89]\tTotal Loss: 0.0371\tMAELoss: 0.0360\tMSSSIM Loss: 0.0372\n",
      "[0/4575][58/89]\tTotal Loss: 0.0498\tMAELoss: 0.0523\tMSSSIM Loss: 0.0493\n",
      "[0/4575][59/89]\tTotal Loss: 0.0314\tMAELoss: 0.0544\tMSSSIM Loss: 0.0270\n",
      "[0/4575][60/89]\tTotal Loss: 0.0254\tMAELoss: 0.0335\tMSSSIM Loss: 0.0239\n",
      "[0/4575][61/89]\tTotal Loss: 0.0206\tMAELoss: 0.0225\tMSSSIM Loss: 0.0203\n",
      "[0/4575][62/89]\tTotal Loss: 0.0324\tMAELoss: 0.0309\tMSSSIM Loss: 0.0327\n",
      "[0/4575][63/89]\tTotal Loss: 0.0346\tMAELoss: 0.0330\tMSSSIM Loss: 0.0349\n",
      "[0/4575][64/89]\tTotal Loss: 0.0370\tMAELoss: 0.0477\tMSSSIM Loss: 0.0349\n",
      "[0/4575][65/89]\tTotal Loss: 0.0326\tMAELoss: 0.0355\tMSSSIM Loss: 0.0321\n",
      "[0/4575][66/89]\tTotal Loss: 0.0365\tMAELoss: 0.0402\tMSSSIM Loss: 0.0358\n",
      "[0/4575][67/89]\tTotal Loss: 0.0305\tMAELoss: 0.0311\tMSSSIM Loss: 0.0303\n",
      "[0/4575][68/89]\tTotal Loss: 0.0224\tMAELoss: 0.0257\tMSSSIM Loss: 0.0218\n",
      "[0/4575][69/89]\tTotal Loss: 0.0299\tMAELoss: 0.0266\tMSSSIM Loss: 0.0306\n",
      "[0/4575][70/89]\tTotal Loss: 0.0343\tMAELoss: 0.0324\tMSSSIM Loss: 0.0347\n",
      "[0/4575][71/89]\tTotal Loss: 0.0218\tMAELoss: 0.0270\tMSSSIM Loss: 0.0208\n",
      "[0/4575][72/89]\tTotal Loss: 0.0255\tMAELoss: 0.0264\tMSSSIM Loss: 0.0253\n",
      "[0/4575][73/89]\tTotal Loss: 0.0295\tMAELoss: 0.0262\tMSSSIM Loss: 0.0301\n",
      "[0/4575][74/89]\tTotal Loss: 0.0292\tMAELoss: 0.0278\tMSSSIM Loss: 0.0294\n",
      "[0/4575][75/89]\tTotal Loss: 0.0306\tMAELoss: 0.0268\tMSSSIM Loss: 0.0313\n",
      "[0/4575][76/89]\tTotal Loss: 0.0296\tMAELoss: 0.0318\tMSSSIM Loss: 0.0292\n",
      "[0/4575][77/89]\tTotal Loss: 0.0215\tMAELoss: 0.0269\tMSSSIM Loss: 0.0205\n",
      "[0/4575][78/89]\tTotal Loss: 0.0323\tMAELoss: 0.0380\tMSSSIM Loss: 0.0313\n",
      "[0/4575][79/89]\tTotal Loss: 0.0220\tMAELoss: 0.0271\tMSSSIM Loss: 0.0210\n",
      "[0/4575][80/89]\tTotal Loss: 0.0320\tMAELoss: 0.0304\tMSSSIM Loss: 0.0323\n",
      "[0/4575][81/89]\tTotal Loss: 0.0260\tMAELoss: 0.0297\tMSSSIM Loss: 0.0253\n",
      "[0/4575][82/89]\tTotal Loss: 0.0228\tMAELoss: 0.0261\tMSSSIM Loss: 0.0221\n",
      "[0/4575][83/89]\tTotal Loss: 0.0248\tMAELoss: 0.0271\tMSSSIM Loss: 0.0243\n",
      "[0/4575][84/89]\tTotal Loss: 0.0349\tMAELoss: 0.0367\tMSSSIM Loss: 0.0346\n",
      "[0/4575][85/89]\tTotal Loss: 0.0230\tMAELoss: 0.0237\tMSSSIM Loss: 0.0229\n",
      "[0/4575][86/89]\tTotal Loss: 0.0254\tMAELoss: 0.0262\tMSSSIM Loss: 0.0252\n",
      "[0/4575][87/89]\tTotal Loss: 0.0222\tMAELoss: 0.0255\tMSSSIM Loss: 0.0216\n",
      "[0/4575][88/89]\tTotal Loss: 0.0214\tMAELoss: 0.0338\tMSSSIM Loss: 0.0190\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "Epoch   493: reducing learning rate of group 0 to 5.0000e-04.\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "Epoch   745: reducing learning rate of group 0 to 2.5000e-04.\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "Epoch   974: reducing learning rate of group 0 to 1.2500e-04.\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "[1000/4575][0/89]\tTotal Loss: 0.0088\tMAELoss: 0.0066\tMSSSIM Loss: 0.0092\n",
      "[1000/4575][1/89]\tTotal Loss: 0.0068\tMAELoss: 0.0070\tMSSSIM Loss: 0.0067\n",
      "[1000/4575][2/89]\tTotal Loss: 0.0055\tMAELoss: 0.0064\tMSSSIM Loss: 0.0054\n",
      "[1000/4575][3/89]\tTotal Loss: 0.0107\tMAELoss: 0.0098\tMSSSIM Loss: 0.0108\n",
      "[1000/4575][4/89]\tTotal Loss: 0.0139\tMAELoss: 0.0093\tMSSSIM Loss: 0.0147\n",
      "[1000/4575][5/89]\tTotal Loss: 0.0061\tMAELoss: 0.0058\tMSSSIM Loss: 0.0061\n",
      "[1000/4575][6/89]\tTotal Loss: 0.0089\tMAELoss: 0.0102\tMSSSIM Loss: 0.0087\n",
      "[1000/4575][7/89]\tTotal Loss: 0.0072\tMAELoss: 0.0080\tMSSSIM Loss: 0.0070\n",
      "[1000/4575][8/89]\tTotal Loss: 0.0054\tMAELoss: 0.0058\tMSSSIM Loss: 0.0053\n",
      "[1000/4575][9/89]\tTotal Loss: 0.0085\tMAELoss: 0.0073\tMSSSIM Loss: 0.0087\n",
      "[1000/4575][10/89]\tTotal Loss: 0.0067\tMAELoss: 0.0068\tMSSSIM Loss: 0.0067\n",
      "[1000/4575][11/89]\tTotal Loss: 0.0109\tMAELoss: 0.0112\tMSSSIM Loss: 0.0108\n",
      "[1000/4575][12/89]\tTotal Loss: 0.0082\tMAELoss: 0.0087\tMSSSIM Loss: 0.0081\n",
      "[1000/4575][13/89]\tTotal Loss: 0.0097\tMAELoss: 0.0085\tMSSSIM Loss: 0.0099\n",
      "[1000/4575][14/89]\tTotal Loss: 0.0106\tMAELoss: 0.0124\tMSSSIM Loss: 0.0103\n",
      "[1000/4575][15/89]\tTotal Loss: 0.0089\tMAELoss: 0.0098\tMSSSIM Loss: 0.0087\n",
      "[1000/4575][16/89]\tTotal Loss: 0.0106\tMAELoss: 0.0125\tMSSSIM Loss: 0.0103\n",
      "[1000/4575][17/89]\tTotal Loss: 0.0073\tMAELoss: 0.0082\tMSSSIM Loss: 0.0071\n",
      "[1000/4575][18/89]\tTotal Loss: 0.0103\tMAELoss: 0.0109\tMSSSIM Loss: 0.0102\n",
      "[1000/4575][19/89]\tTotal Loss: 0.0056\tMAELoss: 0.0062\tMSSSIM Loss: 0.0054\n",
      "[1000/4575][20/89]\tTotal Loss: 0.0080\tMAELoss: 0.0076\tMSSSIM Loss: 0.0081\n",
      "[1000/4575][21/89]\tTotal Loss: 0.0089\tMAELoss: 0.0083\tMSSSIM Loss: 0.0091\n",
      "[1000/4575][22/89]\tTotal Loss: 0.0071\tMAELoss: 0.0068\tMSSSIM Loss: 0.0071\n",
      "[1000/4575][23/89]\tTotal Loss: 0.0091\tMAELoss: 0.0120\tMSSSIM Loss: 0.0086\n",
      "[1000/4575][24/89]\tTotal Loss: 0.0072\tMAELoss: 0.0077\tMSSSIM Loss: 0.0071\n",
      "[1000/4575][25/89]\tTotal Loss: 0.0055\tMAELoss: 0.0056\tMSSSIM Loss: 0.0054\n",
      "[1000/4575][26/89]\tTotal Loss: 0.0062\tMAELoss: 0.0076\tMSSSIM Loss: 0.0060\n",
      "[1000/4575][27/89]\tTotal Loss: 0.0068\tMAELoss: 0.0051\tMSSSIM Loss: 0.0071\n",
      "[1000/4575][28/89]\tTotal Loss: 0.0094\tMAELoss: 0.0122\tMSSSIM Loss: 0.0089\n",
      "[1000/4575][29/89]\tTotal Loss: 0.0061\tMAELoss: 0.0082\tMSSSIM Loss: 0.0057\n",
      "[1000/4575][30/89]\tTotal Loss: 0.0084\tMAELoss: 0.0080\tMSSSIM Loss: 0.0084\n",
      "[1000/4575][31/89]\tTotal Loss: 0.0074\tMAELoss: 0.0081\tMSSSIM Loss: 0.0073\n",
      "[1000/4575][32/89]\tTotal Loss: 0.0080\tMAELoss: 0.0077\tMSSSIM Loss: 0.0081\n",
      "[1000/4575][33/89]\tTotal Loss: 0.0097\tMAELoss: 0.0119\tMSSSIM Loss: 0.0092\n",
      "[1000/4575][34/89]\tTotal Loss: 0.0071\tMAELoss: 0.0071\tMSSSIM Loss: 0.0071\n",
      "[1000/4575][35/89]\tTotal Loss: 0.0121\tMAELoss: 0.0116\tMSSSIM Loss: 0.0122\n",
      "[1000/4575][36/89]\tTotal Loss: 0.0108\tMAELoss: 0.0121\tMSSSIM Loss: 0.0106\n",
      "[1000/4575][37/89]\tTotal Loss: 0.0075\tMAELoss: 0.0079\tMSSSIM Loss: 0.0074\n",
      "[1000/4575][38/89]\tTotal Loss: 0.0065\tMAELoss: 0.0055\tMSSSIM Loss: 0.0067\n",
      "[1000/4575][39/89]\tTotal Loss: 0.0074\tMAELoss: 0.0060\tMSSSIM Loss: 0.0077\n",
      "[1000/4575][40/89]\tTotal Loss: 0.0046\tMAELoss: 0.0052\tMSSSIM Loss: 0.0045\n",
      "[1000/4575][41/89]\tTotal Loss: 0.0128\tMAELoss: 0.0185\tMSSSIM Loss: 0.0117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000/4575][42/89]\tTotal Loss: 0.0098\tMAELoss: 0.0091\tMSSSIM Loss: 0.0099\n",
      "[1000/4575][43/89]\tTotal Loss: 0.0071\tMAELoss: 0.0085\tMSSSIM Loss: 0.0068\n",
      "[1000/4575][44/89]\tTotal Loss: 0.0059\tMAELoss: 0.0062\tMSSSIM Loss: 0.0059\n",
      "[1000/4575][45/89]\tTotal Loss: 0.0097\tMAELoss: 0.0126\tMSSSIM Loss: 0.0091\n",
      "[1000/4575][46/89]\tTotal Loss: 0.0152\tMAELoss: 0.0200\tMSSSIM Loss: 0.0143\n",
      "[1000/4575][47/89]\tTotal Loss: 0.0073\tMAELoss: 0.0062\tMSSSIM Loss: 0.0076\n",
      "[1000/4575][48/89]\tTotal Loss: 0.0077\tMAELoss: 0.0086\tMSSSIM Loss: 0.0075\n",
      "[1000/4575][49/89]\tTotal Loss: 0.0103\tMAELoss: 0.0093\tMSSSIM Loss: 0.0105\n",
      "[1000/4575][50/89]\tTotal Loss: 0.0056\tMAELoss: 0.0053\tMSSSIM Loss: 0.0056\n",
      "[1000/4575][51/89]\tTotal Loss: 0.0064\tMAELoss: 0.0070\tMSSSIM Loss: 0.0063\n",
      "[1000/4575][52/89]\tTotal Loss: 0.0067\tMAELoss: 0.0062\tMSSSIM Loss: 0.0068\n",
      "[1000/4575][53/89]\tTotal Loss: 0.0112\tMAELoss: 0.0105\tMSSSIM Loss: 0.0113\n",
      "[1000/4575][54/89]\tTotal Loss: 0.0132\tMAELoss: 0.0129\tMSSSIM Loss: 0.0133\n",
      "[1000/4575][55/89]\tTotal Loss: 0.0078\tMAELoss: 0.0065\tMSSSIM Loss: 0.0080\n",
      "[1000/4575][56/89]\tTotal Loss: 0.0129\tMAELoss: 0.0111\tMSSSIM Loss: 0.0132\n",
      "[1000/4575][57/89]\tTotal Loss: 0.0081\tMAELoss: 0.0091\tMSSSIM Loss: 0.0079\n",
      "[1000/4575][58/89]\tTotal Loss: 0.0045\tMAELoss: 0.0043\tMSSSIM Loss: 0.0045\n",
      "[1000/4575][59/89]\tTotal Loss: 0.0079\tMAELoss: 0.0079\tMSSSIM Loss: 0.0079\n",
      "[1000/4575][60/89]\tTotal Loss: 0.0075\tMAELoss: 0.0081\tMSSSIM Loss: 0.0074\n",
      "[1000/4575][61/89]\tTotal Loss: 0.0054\tMAELoss: 0.0053\tMSSSIM Loss: 0.0055\n",
      "[1000/4575][62/89]\tTotal Loss: 0.0079\tMAELoss: 0.0080\tMSSSIM Loss: 0.0078\n",
      "[1000/4575][63/89]\tTotal Loss: 0.0107\tMAELoss: 0.0115\tMSSSIM Loss: 0.0106\n",
      "[1000/4575][64/89]\tTotal Loss: 0.0141\tMAELoss: 0.0158\tMSSSIM Loss: 0.0138\n",
      "[1000/4575][65/89]\tTotal Loss: 0.0090\tMAELoss: 0.0102\tMSSSIM Loss: 0.0088\n",
      "[1000/4575][66/89]\tTotal Loss: 0.0072\tMAELoss: 0.0069\tMSSSIM Loss: 0.0073\n",
      "[1000/4575][67/89]\tTotal Loss: 0.0092\tMAELoss: 0.0078\tMSSSIM Loss: 0.0094\n",
      "[1000/4575][68/89]\tTotal Loss: 0.0078\tMAELoss: 0.0097\tMSSSIM Loss: 0.0075\n",
      "[1000/4575][69/89]\tTotal Loss: 0.0090\tMAELoss: 0.0100\tMSSSIM Loss: 0.0088\n",
      "[1000/4575][70/89]\tTotal Loss: 0.0082\tMAELoss: 0.0094\tMSSSIM Loss: 0.0080\n",
      "[1000/4575][71/89]\tTotal Loss: 0.0088\tMAELoss: 0.0129\tMSSSIM Loss: 0.0080\n",
      "[1000/4575][72/89]\tTotal Loss: 0.0100\tMAELoss: 0.0108\tMSSSIM Loss: 0.0099\n",
      "[1000/4575][73/89]\tTotal Loss: 0.0071\tMAELoss: 0.0064\tMSSSIM Loss: 0.0073\n",
      "[1000/4575][74/89]\tTotal Loss: 0.0109\tMAELoss: 0.0110\tMSSSIM Loss: 0.0109\n",
      "[1000/4575][75/89]\tTotal Loss: 0.0088\tMAELoss: 0.0092\tMSSSIM Loss: 0.0088\n",
      "[1000/4575][76/89]\tTotal Loss: 0.0082\tMAELoss: 0.0079\tMSSSIM Loss: 0.0083\n",
      "[1000/4575][77/89]\tTotal Loss: 0.0056\tMAELoss: 0.0052\tMSSSIM Loss: 0.0057\n",
      "[1000/4575][78/89]\tTotal Loss: 0.0074\tMAELoss: 0.0076\tMSSSIM Loss: 0.0074\n",
      "[1000/4575][79/89]\tTotal Loss: 0.0061\tMAELoss: 0.0064\tMSSSIM Loss: 0.0060\n",
      "[1000/4575][80/89]\tTotal Loss: 0.0095\tMAELoss: 0.0083\tMSSSIM Loss: 0.0097\n",
      "[1000/4575][81/89]\tTotal Loss: 0.0075\tMAELoss: 0.0073\tMSSSIM Loss: 0.0075\n",
      "[1000/4575][82/89]\tTotal Loss: 0.0073\tMAELoss: 0.0059\tMSSSIM Loss: 0.0076\n",
      "[1000/4575][83/89]\tTotal Loss: 0.0097\tMAELoss: 0.0093\tMSSSIM Loss: 0.0098\n",
      "[1000/4575][84/89]\tTotal Loss: 0.0081\tMAELoss: 0.0183\tMSSSIM Loss: 0.0062\n",
      "[1000/4575][85/89]\tTotal Loss: 0.0094\tMAELoss: 0.0083\tMSSSIM Loss: 0.0096\n",
      "[1000/4575][86/89]\tTotal Loss: 0.0057\tMAELoss: 0.0063\tMSSSIM Loss: 0.0056\n",
      "[1000/4575][87/89]\tTotal Loss: 0.0123\tMAELoss: 0.0073\tMSSSIM Loss: 0.0132\n",
      "[1000/4575][88/89]\tTotal Loss: 0.0140\tMAELoss: 0.0115\tMSSSIM Loss: 0.0145\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "Epoch  1257: reducing learning rate of group 0 to 6.2500e-05.\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "Epoch  1817: reducing learning rate of group 0 to 3.1250e-05.\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "[2000/4575][0/89]\tTotal Loss: 0.0068\tMAELoss: 0.0054\tMSSSIM Loss: 0.0071\n",
      "[2000/4575][1/89]\tTotal Loss: 0.0075\tMAELoss: 0.0078\tMSSSIM Loss: 0.0075\n",
      "[2000/4575][2/89]\tTotal Loss: 0.0061\tMAELoss: 0.0060\tMSSSIM Loss: 0.0062\n",
      "[2000/4575][3/89]\tTotal Loss: 0.0080\tMAELoss: 0.0074\tMSSSIM Loss: 0.0082\n",
      "[2000/4575][4/89]\tTotal Loss: 0.0069\tMAELoss: 0.0064\tMSSSIM Loss: 0.0071\n",
      "[2000/4575][5/89]\tTotal Loss: 0.0055\tMAELoss: 0.0049\tMSSSIM Loss: 0.0056\n",
      "[2000/4575][6/89]\tTotal Loss: 0.0049\tMAELoss: 0.0061\tMSSSIM Loss: 0.0047\n",
      "[2000/4575][7/89]\tTotal Loss: 0.0038\tMAELoss: 0.0043\tMSSSIM Loss: 0.0037\n",
      "[2000/4575][8/89]\tTotal Loss: 0.0070\tMAELoss: 0.0065\tMSSSIM Loss: 0.0071\n",
      "[2000/4575][9/89]\tTotal Loss: 0.0056\tMAELoss: 0.0079\tMSSSIM Loss: 0.0052\n",
      "[2000/4575][10/89]\tTotal Loss: 0.0093\tMAELoss: 0.0094\tMSSSIM Loss: 0.0092\n",
      "[2000/4575][11/89]\tTotal Loss: 0.0085\tMAELoss: 0.0095\tMSSSIM Loss: 0.0083\n",
      "[2000/4575][12/89]\tTotal Loss: 0.0088\tMAELoss: 0.0071\tMSSSIM Loss: 0.0092\n",
      "[2000/4575][13/89]\tTotal Loss: 0.0092\tMAELoss: 0.0107\tMSSSIM Loss: 0.0089\n",
      "[2000/4575][14/89]\tTotal Loss: 0.0060\tMAELoss: 0.0054\tMSSSIM Loss: 0.0061\n",
      "[2000/4575][15/89]\tTotal Loss: 0.0084\tMAELoss: 0.0105\tMSSSIM Loss: 0.0080\n",
      "[2000/4575][16/89]\tTotal Loss: 0.0095\tMAELoss: 0.0100\tMSSSIM Loss: 0.0095\n",
      "[2000/4575][17/89]\tTotal Loss: 0.0057\tMAELoss: 0.0062\tMSSSIM Loss: 0.0056\n",
      "[2000/4575][18/89]\tTotal Loss: 0.0052\tMAELoss: 0.0054\tMSSSIM Loss: 0.0051\n",
      "[2000/4575][19/89]\tTotal Loss: 0.0079\tMAELoss: 0.0069\tMSSSIM Loss: 0.0081\n",
      "[2000/4575][20/89]\tTotal Loss: 0.0068\tMAELoss: 0.0075\tMSSSIM Loss: 0.0066\n",
      "[2000/4575][21/89]\tTotal Loss: 0.0048\tMAELoss: 0.0048\tMSSSIM Loss: 0.0048\n",
      "[2000/4575][22/89]\tTotal Loss: 0.0120\tMAELoss: 0.0088\tMSSSIM Loss: 0.0126\n",
      "[2000/4575][23/89]\tTotal Loss: 0.0070\tMAELoss: 0.0076\tMSSSIM Loss: 0.0069\n",
      "[2000/4575][24/89]\tTotal Loss: 0.0071\tMAELoss: 0.0083\tMSSSIM Loss: 0.0068\n",
      "[2000/4575][25/89]\tTotal Loss: 0.0091\tMAELoss: 0.0083\tMSSSIM Loss: 0.0093\n",
      "[2000/4575][26/89]\tTotal Loss: 0.0076\tMAELoss: 0.0078\tMSSSIM Loss: 0.0076\n",
      "[2000/4575][27/89]\tTotal Loss: 0.0092\tMAELoss: 0.0090\tMSSSIM Loss: 0.0093\n",
      "[2000/4575][28/89]\tTotal Loss: 0.0062\tMAELoss: 0.0055\tMSSSIM Loss: 0.0063\n",
      "[2000/4575][29/89]\tTotal Loss: 0.0141\tMAELoss: 0.0130\tMSSSIM Loss: 0.0143\n",
      "[2000/4575][30/89]\tTotal Loss: 0.0098\tMAELoss: 0.0074\tMSSSIM Loss: 0.0103\n",
      "[2000/4575][31/89]\tTotal Loss: 0.0073\tMAELoss: 0.0074\tMSSSIM Loss: 0.0073\n",
      "[2000/4575][32/89]\tTotal Loss: 0.0135\tMAELoss: 0.0125\tMSSSIM Loss: 0.0136\n",
      "[2000/4575][33/89]\tTotal Loss: 0.0074\tMAELoss: 0.0072\tMSSSIM Loss: 0.0074\n",
      "[2000/4575][34/89]\tTotal Loss: 0.0078\tMAELoss: 0.0071\tMSSSIM Loss: 0.0080\n",
      "[2000/4575][35/89]\tTotal Loss: 0.0061\tMAELoss: 0.0062\tMSSSIM Loss: 0.0060\n",
      "[2000/4575][36/89]\tTotal Loss: 0.0090\tMAELoss: 0.0079\tMSSSIM Loss: 0.0092\n",
      "[2000/4575][37/89]\tTotal Loss: 0.0081\tMAELoss: 0.0094\tMSSSIM Loss: 0.0078\n",
      "[2000/4575][38/89]\tTotal Loss: 0.0069\tMAELoss: 0.0095\tMSSSIM Loss: 0.0064\n",
      "[2000/4575][39/89]\tTotal Loss: 0.0074\tMAELoss: 0.0083\tMSSSIM Loss: 0.0072\n",
      "[2000/4575][40/89]\tTotal Loss: 0.0072\tMAELoss: 0.0077\tMSSSIM Loss: 0.0071\n",
      "[2000/4575][41/89]\tTotal Loss: 0.0095\tMAELoss: 0.0099\tMSSSIM Loss: 0.0094\n",
      "[2000/4575][42/89]\tTotal Loss: 0.0055\tMAELoss: 0.0052\tMSSSIM Loss: 0.0056\n",
      "[2000/4575][43/89]\tTotal Loss: 0.0071\tMAELoss: 0.0078\tMSSSIM Loss: 0.0070\n",
      "[2000/4575][44/89]\tTotal Loss: 0.0075\tMAELoss: 0.0078\tMSSSIM Loss: 0.0074\n",
      "[2000/4575][45/89]\tTotal Loss: 0.0054\tMAELoss: 0.0063\tMSSSIM Loss: 0.0052\n",
      "[2000/4575][46/89]\tTotal Loss: 0.0077\tMAELoss: 0.0067\tMSSSIM Loss: 0.0079\n",
      "[2000/4575][47/89]\tTotal Loss: 0.0065\tMAELoss: 0.0081\tMSSSIM Loss: 0.0062\n",
      "[2000/4575][48/89]\tTotal Loss: 0.0105\tMAELoss: 0.0102\tMSSSIM Loss: 0.0106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000/4575][49/89]\tTotal Loss: 0.0059\tMAELoss: 0.0081\tMSSSIM Loss: 0.0055\n",
      "[2000/4575][50/89]\tTotal Loss: 0.0076\tMAELoss: 0.0078\tMSSSIM Loss: 0.0076\n",
      "[2000/4575][51/89]\tTotal Loss: 0.0055\tMAELoss: 0.0053\tMSSSIM Loss: 0.0055\n",
      "[2000/4575][52/89]\tTotal Loss: 0.0102\tMAELoss: 0.0093\tMSSSIM Loss: 0.0104\n",
      "[2000/4575][53/89]\tTotal Loss: 0.0100\tMAELoss: 0.0104\tMSSSIM Loss: 0.0100\n",
      "[2000/4575][54/89]\tTotal Loss: 0.0076\tMAELoss: 0.0091\tMSSSIM Loss: 0.0073\n",
      "[2000/4575][55/89]\tTotal Loss: 0.0070\tMAELoss: 0.0083\tMSSSIM Loss: 0.0067\n",
      "[2000/4575][56/89]\tTotal Loss: 0.0080\tMAELoss: 0.0100\tMSSSIM Loss: 0.0076\n",
      "[2000/4575][57/89]\tTotal Loss: 0.0135\tMAELoss: 0.0252\tMSSSIM Loss: 0.0113\n",
      "[2000/4575][58/89]\tTotal Loss: 0.0058\tMAELoss: 0.0064\tMSSSIM Loss: 0.0057\n",
      "[2000/4575][59/89]\tTotal Loss: 0.0088\tMAELoss: 0.0113\tMSSSIM Loss: 0.0084\n",
      "[2000/4575][60/89]\tTotal Loss: 0.0072\tMAELoss: 0.0084\tMSSSIM Loss: 0.0070\n",
      "[2000/4575][61/89]\tTotal Loss: 0.0077\tMAELoss: 0.0090\tMSSSIM Loss: 0.0075\n",
      "[2000/4575][62/89]\tTotal Loss: 0.0061\tMAELoss: 0.0063\tMSSSIM Loss: 0.0060\n",
      "[2000/4575][63/89]\tTotal Loss: 0.0125\tMAELoss: 0.0132\tMSSSIM Loss: 0.0124\n",
      "[2000/4575][64/89]\tTotal Loss: 0.0091\tMAELoss: 0.0065\tMSSSIM Loss: 0.0096\n",
      "[2000/4575][65/89]\tTotal Loss: 0.0075\tMAELoss: 0.0070\tMSSSIM Loss: 0.0075\n",
      "[2000/4575][66/89]\tTotal Loss: 0.0089\tMAELoss: 0.0089\tMSSSIM Loss: 0.0089\n",
      "[2000/4575][67/89]\tTotal Loss: 0.0079\tMAELoss: 0.0069\tMSSSIM Loss: 0.0081\n",
      "[2000/4575][68/89]\tTotal Loss: 0.0096\tMAELoss: 0.0093\tMSSSIM Loss: 0.0096\n",
      "[2000/4575][69/89]\tTotal Loss: 0.0057\tMAELoss: 0.0053\tMSSSIM Loss: 0.0057\n",
      "[2000/4575][70/89]\tTotal Loss: 0.0106\tMAELoss: 0.0097\tMSSSIM Loss: 0.0108\n",
      "[2000/4575][71/89]\tTotal Loss: 0.0095\tMAELoss: 0.0089\tMSSSIM Loss: 0.0096\n",
      "[2000/4575][72/89]\tTotal Loss: 0.0063\tMAELoss: 0.0065\tMSSSIM Loss: 0.0062\n",
      "[2000/4575][73/89]\tTotal Loss: 0.0069\tMAELoss: 0.0075\tMSSSIM Loss: 0.0068\n",
      "[2000/4575][74/89]\tTotal Loss: 0.0114\tMAELoss: 0.0125\tMSSSIM Loss: 0.0112\n",
      "[2000/4575][75/89]\tTotal Loss: 0.0081\tMAELoss: 0.0070\tMSSSIM Loss: 0.0083\n",
      "[2000/4575][76/89]\tTotal Loss: 0.0077\tMAELoss: 0.0080\tMSSSIM Loss: 0.0076\n",
      "[2000/4575][77/89]\tTotal Loss: 0.0106\tMAELoss: 0.0125\tMSSSIM Loss: 0.0103\n",
      "[2000/4575][78/89]\tTotal Loss: 0.0105\tMAELoss: 0.0142\tMSSSIM Loss: 0.0098\n",
      "[2000/4575][79/89]\tTotal Loss: 0.0076\tMAELoss: 0.0068\tMSSSIM Loss: 0.0078\n",
      "[2000/4575][80/89]\tTotal Loss: 0.0125\tMAELoss: 0.0133\tMSSSIM Loss: 0.0124\n",
      "[2000/4575][81/89]\tTotal Loss: 0.0046\tMAELoss: 0.0047\tMSSSIM Loss: 0.0045\n",
      "[2000/4575][82/89]\tTotal Loss: 0.0071\tMAELoss: 0.0088\tMSSSIM Loss: 0.0068\n",
      "[2000/4575][83/89]\tTotal Loss: 0.0055\tMAELoss: 0.0067\tMSSSIM Loss: 0.0053\n",
      "[2000/4575][84/89]\tTotal Loss: 0.0110\tMAELoss: 0.0147\tMSSSIM Loss: 0.0103\n",
      "[2000/4575][85/89]\tTotal Loss: 0.0066\tMAELoss: 0.0058\tMSSSIM Loss: 0.0068\n",
      "[2000/4575][86/89]\tTotal Loss: 0.0082\tMAELoss: 0.0096\tMSSSIM Loss: 0.0079\n",
      "[2000/4575][87/89]\tTotal Loss: 0.0045\tMAELoss: 0.0044\tMSSSIM Loss: 0.0045\n",
      "[2000/4575][88/89]\tTotal Loss: 0.0048\tMAELoss: 0.0050\tMSSSIM Loss: 0.0047\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "3.125e-05\n",
      "Epoch  2046: reducing learning rate of group 0 to 1.5625e-05.\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "1.5625e-05\n",
      "Epoch  2275: reducing learning rate of group 0 to 1.0000e-05.\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "[3000/4575][0/89]\tTotal Loss: 0.0073\tMAELoss: 0.0074\tMSSSIM Loss: 0.0073\n",
      "[3000/4575][1/89]\tTotal Loss: 0.0054\tMAELoss: 0.0068\tMSSSIM Loss: 0.0051\n",
      "[3000/4575][2/89]\tTotal Loss: 0.0133\tMAELoss: 0.0160\tMSSSIM Loss: 0.0127\n",
      "[3000/4575][3/89]\tTotal Loss: 0.0088\tMAELoss: 0.0092\tMSSSIM Loss: 0.0087\n",
      "[3000/4575][4/89]\tTotal Loss: 0.0053\tMAELoss: 0.0046\tMSSSIM Loss: 0.0055\n",
      "[3000/4575][5/89]\tTotal Loss: 0.0068\tMAELoss: 0.0100\tMSSSIM Loss: 0.0062\n",
      "[3000/4575][6/89]\tTotal Loss: 0.0097\tMAELoss: 0.0101\tMSSSIM Loss: 0.0096\n",
      "[3000/4575][7/89]\tTotal Loss: 0.0062\tMAELoss: 0.0068\tMSSSIM Loss: 0.0061\n",
      "[3000/4575][8/89]\tTotal Loss: 0.0075\tMAELoss: 0.0084\tMSSSIM Loss: 0.0073\n",
      "[3000/4575][9/89]\tTotal Loss: 0.0054\tMAELoss: 0.0058\tMSSSIM Loss: 0.0053\n",
      "[3000/4575][10/89]\tTotal Loss: 0.0075\tMAELoss: 0.0073\tMSSSIM Loss: 0.0075\n",
      "[3000/4575][11/89]\tTotal Loss: 0.0039\tMAELoss: 0.0042\tMSSSIM Loss: 0.0039\n",
      "[3000/4575][12/89]\tTotal Loss: 0.0073\tMAELoss: 0.0071\tMSSSIM Loss: 0.0073\n",
      "[3000/4575][13/89]\tTotal Loss: 0.0067\tMAELoss: 0.0056\tMSSSIM Loss: 0.0069\n",
      "[3000/4575][14/89]\tTotal Loss: 0.0077\tMAELoss: 0.0058\tMSSSIM Loss: 0.0081\n",
      "[3000/4575][15/89]\tTotal Loss: 0.0084\tMAELoss: 0.0134\tMSSSIM Loss: 0.0075\n",
      "[3000/4575][16/89]\tTotal Loss: 0.0069\tMAELoss: 0.0061\tMSSSIM Loss: 0.0071\n",
      "[3000/4575][17/89]\tTotal Loss: 0.0095\tMAELoss: 0.0102\tMSSSIM Loss: 0.0094\n",
      "[3000/4575][18/89]\tTotal Loss: 0.0105\tMAELoss: 0.0100\tMSSSIM Loss: 0.0106\n",
      "[3000/4575][19/89]\tTotal Loss: 0.0092\tMAELoss: 0.0093\tMSSSIM Loss: 0.0092\n",
      "[3000/4575][20/89]\tTotal Loss: 0.0137\tMAELoss: 0.0128\tMSSSIM Loss: 0.0139\n",
      "[3000/4575][21/89]\tTotal Loss: 0.0119\tMAELoss: 0.0105\tMSSSIM Loss: 0.0121\n",
      "[3000/4575][22/89]\tTotal Loss: 0.0096\tMAELoss: 0.0072\tMSSSIM Loss: 0.0101\n",
      "[3000/4575][23/89]\tTotal Loss: 0.0072\tMAELoss: 0.0066\tMSSSIM Loss: 0.0074\n",
      "[3000/4575][24/89]\tTotal Loss: 0.0103\tMAELoss: 0.0124\tMSSSIM Loss: 0.0099\n",
      "[3000/4575][25/89]\tTotal Loss: 0.0097\tMAELoss: 0.0111\tMSSSIM Loss: 0.0094\n",
      "[3000/4575][26/89]\tTotal Loss: 0.0083\tMAELoss: 0.0067\tMSSSIM Loss: 0.0087\n",
      "[3000/4575][27/89]\tTotal Loss: 0.0058\tMAELoss: 0.0069\tMSSSIM Loss: 0.0056\n",
      "[3000/4575][28/89]\tTotal Loss: 0.0045\tMAELoss: 0.0042\tMSSSIM Loss: 0.0045\n",
      "[3000/4575][29/89]\tTotal Loss: 0.0066\tMAELoss: 0.0082\tMSSSIM Loss: 0.0063\n",
      "[3000/4575][30/89]\tTotal Loss: 0.0062\tMAELoss: 0.0058\tMSSSIM Loss: 0.0063\n",
      "[3000/4575][31/89]\tTotal Loss: 0.0062\tMAELoss: 0.0075\tMSSSIM Loss: 0.0059\n",
      "[3000/4575][32/89]\tTotal Loss: 0.0142\tMAELoss: 0.0230\tMSSSIM Loss: 0.0126\n",
      "[3000/4575][33/89]\tTotal Loss: 0.0084\tMAELoss: 0.0094\tMSSSIM Loss: 0.0083\n",
      "[3000/4575][34/89]\tTotal Loss: 0.0071\tMAELoss: 0.0064\tMSSSIM Loss: 0.0073\n",
      "[3000/4575][35/89]\tTotal Loss: 0.0095\tMAELoss: 0.0102\tMSSSIM Loss: 0.0094\n",
      "[3000/4575][36/89]\tTotal Loss: 0.0085\tMAELoss: 0.0081\tMSSSIM Loss: 0.0086\n",
      "[3000/4575][37/89]\tTotal Loss: 0.0073\tMAELoss: 0.0084\tMSSSIM Loss: 0.0071\n",
      "[3000/4575][38/89]\tTotal Loss: 0.0060\tMAELoss: 0.0059\tMSSSIM Loss: 0.0060\n",
      "[3000/4575][39/89]\tTotal Loss: 0.0088\tMAELoss: 0.0067\tMSSSIM Loss: 0.0092\n",
      "[3000/4575][40/89]\tTotal Loss: 0.0079\tMAELoss: 0.0081\tMSSSIM Loss: 0.0079\n",
      "[3000/4575][41/89]\tTotal Loss: 0.0057\tMAELoss: 0.0069\tMSSSIM Loss: 0.0055\n",
      "[3000/4575][42/89]\tTotal Loss: 0.0072\tMAELoss: 0.0070\tMSSSIM Loss: 0.0073\n",
      "[3000/4575][43/89]\tTotal Loss: 0.0061\tMAELoss: 0.0074\tMSSSIM Loss: 0.0059\n",
      "[3000/4575][44/89]\tTotal Loss: 0.0080\tMAELoss: 0.0076\tMSSSIM Loss: 0.0080\n",
      "[3000/4575][45/89]\tTotal Loss: 0.0066\tMAELoss: 0.0075\tMSSSIM Loss: 0.0064\n",
      "[3000/4575][46/89]\tTotal Loss: 0.0078\tMAELoss: 0.0086\tMSSSIM Loss: 0.0076\n",
      "[3000/4575][47/89]\tTotal Loss: 0.0048\tMAELoss: 0.0042\tMSSSIM Loss: 0.0049\n",
      "[3000/4575][48/89]\tTotal Loss: 0.0070\tMAELoss: 0.0061\tMSSSIM Loss: 0.0072\n",
      "[3000/4575][49/89]\tTotal Loss: 0.0091\tMAELoss: 0.0105\tMSSSIM Loss: 0.0089\n",
      "[3000/4575][50/89]\tTotal Loss: 0.0073\tMAELoss: 0.0079\tMSSSIM Loss: 0.0071\n",
      "[3000/4575][51/89]\tTotal Loss: 0.0070\tMAELoss: 0.0081\tMSSSIM Loss: 0.0068\n",
      "[3000/4575][52/89]\tTotal Loss: 0.0061\tMAELoss: 0.0085\tMSSSIM Loss: 0.0057\n",
      "[3000/4575][53/89]\tTotal Loss: 0.0060\tMAELoss: 0.0064\tMSSSIM Loss: 0.0059\n",
      "[3000/4575][54/89]\tTotal Loss: 0.0051\tMAELoss: 0.0049\tMSSSIM Loss: 0.0051\n",
      "[3000/4575][55/89]\tTotal Loss: 0.0054\tMAELoss: 0.0053\tMSSSIM Loss: 0.0055\n",
      "[3000/4575][56/89]\tTotal Loss: 0.0098\tMAELoss: 0.0133\tMSSSIM Loss: 0.0092\n",
      "[3000/4575][57/89]\tTotal Loss: 0.0061\tMAELoss: 0.0067\tMSSSIM Loss: 0.0060\n",
      "[3000/4575][58/89]\tTotal Loss: 0.0058\tMAELoss: 0.0080\tMSSSIM Loss: 0.0054\n",
      "[3000/4575][59/89]\tTotal Loss: 0.0076\tMAELoss: 0.0078\tMSSSIM Loss: 0.0075\n",
      "[3000/4575][60/89]\tTotal Loss: 0.0049\tMAELoss: 0.0055\tMSSSIM Loss: 0.0048\n",
      "[3000/4575][61/89]\tTotal Loss: 0.0088\tMAELoss: 0.0095\tMSSSIM Loss: 0.0087\n",
      "[3000/4575][62/89]\tTotal Loss: 0.0089\tMAELoss: 0.0098\tMSSSIM Loss: 0.0088\n",
      "[3000/4575][63/89]\tTotal Loss: 0.0057\tMAELoss: 0.0057\tMSSSIM Loss: 0.0057\n",
      "[3000/4575][64/89]\tTotal Loss: 0.0072\tMAELoss: 0.0074\tMSSSIM Loss: 0.0072\n",
      "[3000/4575][65/89]\tTotal Loss: 0.0090\tMAELoss: 0.0086\tMSSSIM Loss: 0.0091\n",
      "[3000/4575][66/89]\tTotal Loss: 0.0062\tMAELoss: 0.0067\tMSSSIM Loss: 0.0061\n",
      "[3000/4575][67/89]\tTotal Loss: 0.0123\tMAELoss: 0.0143\tMSSSIM Loss: 0.0119\n",
      "[3000/4575][68/89]\tTotal Loss: 0.0096\tMAELoss: 0.0087\tMSSSIM Loss: 0.0097\n",
      "[3000/4575][69/89]\tTotal Loss: 0.0092\tMAELoss: 0.0100\tMSSSIM Loss: 0.0091\n",
      "[3000/4575][70/89]\tTotal Loss: 0.0061\tMAELoss: 0.0074\tMSSSIM Loss: 0.0058\n",
      "[3000/4575][71/89]\tTotal Loss: 0.0068\tMAELoss: 0.0063\tMSSSIM Loss: 0.0069\n",
      "[3000/4575][72/89]\tTotal Loss: 0.0099\tMAELoss: 0.0096\tMSSSIM Loss: 0.0100\n",
      "[3000/4575][73/89]\tTotal Loss: 0.0082\tMAELoss: 0.0093\tMSSSIM Loss: 0.0080\n",
      "[3000/4575][74/89]\tTotal Loss: 0.0094\tMAELoss: 0.0094\tMSSSIM Loss: 0.0094\n",
      "[3000/4575][75/89]\tTotal Loss: 0.0059\tMAELoss: 0.0056\tMSSSIM Loss: 0.0059\n",
      "[3000/4575][76/89]\tTotal Loss: 0.0070\tMAELoss: 0.0078\tMSSSIM Loss: 0.0068\n",
      "[3000/4575][77/89]\tTotal Loss: 0.0082\tMAELoss: 0.0096\tMSSSIM Loss: 0.0079\n",
      "[3000/4575][78/89]\tTotal Loss: 0.0072\tMAELoss: 0.0077\tMSSSIM Loss: 0.0071\n",
      "[3000/4575][79/89]\tTotal Loss: 0.0067\tMAELoss: 0.0084\tMSSSIM Loss: 0.0064\n",
      "[3000/4575][80/89]\tTotal Loss: 0.0090\tMAELoss: 0.0097\tMSSSIM Loss: 0.0089\n",
      "[3000/4575][81/89]\tTotal Loss: 0.0080\tMAELoss: 0.0088\tMSSSIM Loss: 0.0078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3000/4575][82/89]\tTotal Loss: 0.0069\tMAELoss: 0.0070\tMSSSIM Loss: 0.0069\n",
      "[3000/4575][83/89]\tTotal Loss: 0.0042\tMAELoss: 0.0042\tMSSSIM Loss: 0.0041\n",
      "[3000/4575][84/89]\tTotal Loss: 0.0075\tMAELoss: 0.0090\tMSSSIM Loss: 0.0073\n",
      "[3000/4575][85/89]\tTotal Loss: 0.0090\tMAELoss: 0.0079\tMSSSIM Loss: 0.0091\n",
      "[3000/4575][86/89]\tTotal Loss: 0.0118\tMAELoss: 0.0119\tMSSSIM Loss: 0.0118\n",
      "[3000/4575][87/89]\tTotal Loss: 0.0054\tMAELoss: 0.0072\tMSSSIM Loss: 0.0051\n",
      "[3000/4575][88/89]\tTotal Loss: 0.0051\tMAELoss: 0.0059\tMSSSIM Loss: 0.0050\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "# Data augmentation transformations\n",
    "data_transforms = {\n",
    "    'train_transforms': tvtransforms.Compose([\n",
    "    custom_transforms.RandomAutocontrast(sample_keys_images, cutoff_limits=(0.1,0.1)),\n",
    "    custom_transforms.CenterCrop(sample_keys_images,256),\n",
    "    custom_transforms.RandomHorizontalFlip(sample_keys_images, 0.5),\n",
    "    custom_transforms.RandomAffine(sample_keys_images, degrees=10,translate=(0.1,0.1),scale=(0.9,1.1)),\n",
    "    custom_transforms.ToTensor(sample_keys_images),\n",
    "    custom_transforms.ImageComplement(sample_keys_images),\n",
    "    ])\n",
    "      ,\n",
    "    'test_transforms': tvtransforms.Compose([\n",
    "      custom_transforms.ToTensor(sample_keys_images),\n",
    "    ]),\n",
    "    }\n",
    "\n",
    "# K-FOLD VALIDATION\n",
    "splits=KFold(n_splits=split_k_folds,shuffle=True,random_state=42)\n",
    "foldperf={}\n",
    "\n",
    "print(target_key)\n",
    "dataset = datasets.JSRT_CXR(data_normal, data_BSE, transform=None)\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    history = {'loss':[], 'ssim_acc':[]}\n",
    "    \n",
    "    # Subset sample from dataset\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    dl_training = DataLoader(datasets.JSRT_CXR(data_normal, data_BSE, transform=data_transforms['train_transforms']),\n",
    "                             batch_size=_batch_size, sampler=train_sampler, num_workers=0)\n",
    "    dl_validation = DataLoader(datasets.JSRT_CXR(data_normal, data_BSE, transform=data_transforms['train_transforms']),\n",
    "                             batch_size=_batch_size, sampler=test_sampler, num_workers=0)\n",
    "    \n",
    "    # Print example of transformed image\n",
    "    for count, sample in enumerate(dl_training):\n",
    "        image = sample[\"source\"][0,:]\n",
    "        image = torch.squeeze(image)\n",
    "        plt.imshow(image)\n",
    "        if count == 0:\n",
    "            break\n",
    "\n",
    "    ## Implementation of network and losses\n",
    "    input_array_size = (_batch_size, 1, image_spatial_size[0], image_spatial_size[1])\n",
    "    net = RajaramanModel.ResNet_BS(input_array_size)\n",
    "    # Initialise weights\n",
    "    net.apply(weights_init)\n",
    "\n",
    "    # Multi-GPU\n",
    "    if (device.type == 'cuda') and (ngpu > 1):\n",
    "        print(\"Neural Net on GPU\")\n",
    "        net = nn.DataParallel(net, list(range(ngpu)))\n",
    "    net = net.to(device)\n",
    "\n",
    "    # Optimiser\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr_ini, betas=(beta1, beta2))\n",
    "\n",
    "    # Learning Rate Scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                                     factor=0.5, patience=int(10*epoch_factor), \n",
    "                                                     threshold=0.0001, min_lr=0.00001, verbose=True)\n",
    "    \n",
    "    # For each epoch\n",
    "    epochs_list = []\n",
    "    img_list = []\n",
    "    training_loss_list = []\n",
    "    reals_shown = []\n",
    "    #validation_loss_per_epoch_list = []\n",
    "    #training_loss_per_epoch_list = []\n",
    "    loss_per_epoch={\"training\":[], \"validation\":[]}\n",
    "    ssim_average={\"training\":[], \"validation\":[]}\n",
    "\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    if flag_load_previous_save:\n",
    "        if os.path.isfile(PATH_SAVE_NETWORK_INTERMEDIATE):\n",
    "            print(\"=> loading checkpoint '{}'\".format(PATH_SAVE_NETWORK_INTERMEDIATE))\n",
    "            checkpoint = torch.load(PATH_SAVE_NETWORK_INTERMEDIATE)\n",
    "            start_epoch = checkpoint['epoch_next']\n",
    "            reals_shown_now = checkpoint['reals_shown']\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            net.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {}, reals shown {})\".format(PATH_SAVE_NETWORK_INTERMEDIATE, \n",
    "                                                                                start_epoch, reals_shown_now))\n",
    "            print(scheduler)\n",
    "        else:\n",
    "            print(\"=> NO CHECKPOINT FOUND AT '{}'\" .format(PATH_SAVE_NETWORK_INTERMEDIATE))\n",
    "            raise RuntimeError(\"No checkpoint found at specified path.\")\n",
    "    else:\n",
    "        print(\"FLAG: NO CHECKPOINT LOADED.\")\n",
    "        reals_shown_now = 0\n",
    "        start_epoch=0\n",
    "\n",
    "    # Loop variables\n",
    "    flag_break = False # when debugging, this will automatically go to True\n",
    "    iters = 0\n",
    "    net.train()\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = True\n",
    "    for epoch in range(start_epoch, total_num_epochs):\n",
    "        print(optimizer.param_groups[0]['lr'])\n",
    "        sum_loss_in_epoch = 0\n",
    "        for i, data in enumerate(dl_training):\n",
    "            # Training\n",
    "            net.zero_grad()\n",
    "            noisy_data = data[original_key].to(device)\n",
    "            cleaned_data = net(noisy_data)\n",
    "            loss, maeloss, msssim_loss = criterion_Rajaraman(cleaned_data, data[target_key].to(device))\n",
    "            loss.backward() # calculate gradients\n",
    "            optimizer.step() # optimiser step along gradients\n",
    "\n",
    "            # Output training stats\n",
    "            if epoch % 1000 == 0:\n",
    "                print('[%d/%d][%d/%d]\\tTotal Loss: %.4f\\tMAELoss: %.4f\\tMSSSIM Loss: %.4f'\n",
    "                      % (epoch, total_num_epochs, i, len(dl_training),\n",
    "                         loss.item(), maeloss.item(), msssim_loss.item()))\n",
    "            # Record generator output\n",
    "            #if reals_shown_now%(100*_batch_size)==0:\n",
    "            #    with torch.no_grad():\n",
    "            #        val_cleaned = net(fixed_val_sample[\"source\"].to(device)).detach().cpu()\n",
    "            #    print(\"Printing to img_list\")\n",
    "            #    img_list.append(vutils.make_grid(val_cleaned, padding=2, normalize=True))\n",
    "            #iters +=1\n",
    "            #if flag_debug and iters>=10:\n",
    "            #    flag_break = True\n",
    "            #    break\n",
    "\n",
    "            # Running counter of reals shown\n",
    "            reals_shown_now += _batch_size\n",
    "            reals_shown.append(reals_shown_now)\n",
    "\n",
    "            # Training loss list for loss per minibatch\n",
    "            training_loss_list.append(loss.item()) # training loss\n",
    "            sum_loss_in_epoch += loss.item()*len(cleaned_data)\n",
    "        # Turn the sum loss in epoch into a loss-per-epoch\n",
    "        loss_per_epoch[\"training\"].append(sum_loss_in_epoch/len(train_idx))\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Training Accuracy per EPOCH\n",
    "            ssim_training_list = []\n",
    "            for train_count, data in enumerate(dl_training):\n",
    "                noisy_training_data = data[original_key].to(device)\n",
    "                true_training_data = data[target_key]\n",
    "                cleaned_training_data = net(noisy_training_data)\n",
    "\n",
    "                for ii, image in enumerate(cleaned_training_data):\n",
    "                    clean_training_numpy = image.cpu().detach().numpy()\n",
    "                    true_training_numpy = true_training_data[ii].numpy()\n",
    "                    clean_training_numpy = np.moveaxis(clean_training_numpy, 0, -1)\n",
    "                    true_training_numpy = np.moveaxis(true_training_numpy, 0, -1)\n",
    "                    ssim_training = skimage.metrics.structural_similarity(clean_training_numpy, true_training_numpy, multichannel=True)\n",
    "                    ssim_training_list.append(ssim_training) # SSIM per image\n",
    "            ssim_average[\"training\"].append(np.mean(ssim_training_list))\n",
    "\n",
    "            # Validation Loss and Accuracy per EPOCH\n",
    "            sum_loss_in_epoch =0\n",
    "            ssim_val_list = []\n",
    "            for val_count, sample in enumerate(dl_validation):\n",
    "                noisy_val_data = sample[original_key].to(device)\n",
    "                cleaned_val_data = net(noisy_val_data)\n",
    "\n",
    "                # Loss\n",
    "                true_val_data = sample[target_key]\n",
    "                val_loss, maeloss, msssim_loss = criterion_Rajaraman(cleaned_val_data, true_val_data.to(device))\n",
    "                sum_loss_in_epoch += val_loss.item()*len(cleaned_val_data)\n",
    "\n",
    "                # Accuracy\n",
    "                for ii, image in enumerate(cleaned_val_data):\n",
    "                    clean_val_numpy = image.cpu().detach().numpy()\n",
    "                    true_val_numpy = true_val_data[ii].numpy()\n",
    "                    clean_val_numpy = np.moveaxis(clean_val_numpy, 0, -1)\n",
    "                    true_val_numpy = np.moveaxis(true_val_numpy, 0, -1)\n",
    "                    ssim_val = skimage.metrics.structural_similarity(clean_val_numpy, true_val_numpy, multichannel=True)\n",
    "                    ssim_val_list.append(ssim_val) # SSIM per image\n",
    "            # After considering all validation images\n",
    "            loss_per_epoch[\"validation\"].append(sum_loss_in_epoch/len(val_idx))\n",
    "            ssim_average[\"validation\"].append(np.mean(ssim_val_list))\n",
    "        epochs_list.append(epoch)\n",
    "        # LR Scheduler after epoch\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Save the network in indications\n",
    "        #if epoch % 5 == 0:\n",
    "        #    if not flag_debug:\n",
    "        #        #torch.save(net.state_dict(), PATH_SAVE_NETWORK_INTERMEDIATE)\n",
    "        #        torch.save({\n",
    "        #        'epochs_completed': epoch+1,\n",
    "        #        'epoch_next': epoch+1,\n",
    "        #        'model_state_dict': net.state_dict(),\n",
    "        #        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        #        'loss': loss,\n",
    "        #        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        #        'reals_shown': reals_shown_now\n",
    "        #        }, PATH_SAVE_NETWORK_INTERMEDIATE)\n",
    "        #        print(\"Saved Intermediate: \"+ str(PATH_SAVE_NETWORK_INTERMEDIATE))\n",
    "        #if flag_break:\n",
    "        #    break\n",
    "        \n",
    "    #After all epochs, save results:\n",
    "    history[\"loss\"] = loss_per_epoch\n",
    "    history[\"ssim_acc\"] = ssim_average\n",
    "    \n",
    "    foldperf['fold{}'.format(fold+1)] = history\n",
    "    \n",
    "    # Final Save for each fold\n",
    "    if not flag_debug:\n",
    "        torch.save({\n",
    "                'epochs_completed': epoch+1,\n",
    "                'epoch_next': epoch+1,\n",
    "                'model_state_dict': net.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'reals_shown': reals_shown_now\n",
    "                }, PATH_SAVE_NETWORK_INTERMEDIATE.format(fold+1))\n",
    "        print(\"Saved Intermediate: \"+ str(PATH_SAVE_NETWORK_INTERMEDIATE.format(fold+1)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a437c08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Training complete\")\n",
    "# Find the best fold\n",
    "\n",
    "\n",
    "# AVERAGE PERFORMANCE\n",
    "testl_f,tl_f,testa_f,ta_f=[],[],[],[]\n",
    "k=10\n",
    "for f in range(1,k+1):\n",
    "    tl_f.append(np.mean(foldperf['fold{}'.format(f)]['loss']['training']))\n",
    "    testl_f.append(np.mean(foldperf['fold{}'.format(f)]['loss']['validation']))\n",
    "\n",
    "    ta_f.append(np.mean(foldperf['fold{}'.format(f)]['ssim_acc']['training']))\n",
    "    testa_f.append(np.mean(foldperf['fold{}'.format(f)]['ssim_acc']['validation']))\n",
    "\n",
    "print('Performance of {} fold cross validation'.format(k))\n",
    "print(\"Average Training Loss: {:.3f} \\t Average Test Loss: {:.3f} \\t Average Training Acc: {:.2f} \\t Average Test Acc: {:.2f}\"\n",
    "      .format(np.mean(tl_f),np.mean(testl_f),np.mean(ta_f),np.mean(testa_f)))\n",
    "print(\"Best fold: {}\".format(np.argmax(testa_f)+1))\n",
    "\n",
    "# Averaging accuracy and loss\n",
    "diz_ep = {'train_loss_ep':[],'test_loss_ep':[],'train_acc_ep':[],'test_acc_ep':[]}\n",
    "for i in range(total_num_epochs):\n",
    "    diz_ep['train_loss_ep'].append(np.mean([foldperf['fold{}'.format(f+1)]['loss']['training'][i] for f in range(k)]))\n",
    "    diz_ep['test_loss_ep'].append(np.mean([foldperf['fold{}'.format(f+1)]['loss']['validation'][i] for f in range(k)]))\n",
    "    diz_ep['train_acc_ep'].append(np.mean([foldperf['fold{}'.format(f+1)]['ssim_acc']['training'][i] for f in range(k)]))\n",
    "    diz_ep['test_acc_ep'].append(np.mean([foldperf['fold{}'.format(f+1)]['ssim_acc']['validation'][i] for f in range(k)]))\n",
    "\n",
    "# Plot losses\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.semilogy(diz_ep['train_loss_ep'], label='Train')\n",
    "plt.semilogy(diz_ep['test_loss_ep'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "#plt.grid()\n",
    "plt.legend()\n",
    "if not flag_debug:\n",
    "    plt.savefig(os.path.join(output_save_directory, current_date + \"_loss\"+\".png\"))\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracies\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.semilogy(diz_ep['train_acc_ep'], label='Train')\n",
    "plt.semilogy(diz_ep['test_acc_ep'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('SSIM')\n",
    "#plt.grid()\n",
    "plt.legend()\n",
    "if not flag_debug:\n",
    "    plt.savefig(os.path.join(output_save_directory, current_date + \"_accuracy\"+\".png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0f3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "current_date=datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Accuracy\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"SSIM\")\n",
    "plt.plot(epochs_list, ssim_average[\"training\"], label='training')\n",
    "plt.plot(epochs_list, ssim_average[\"validation\"] , label='validation')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"SSIM\")\n",
    "plt.legend()\n",
    "if not flag_debug:\n",
    "    plt.savefig(os.path.join(output_save_directory, current_date + \"_accuracy\"+\".png\"))\n",
    "    \n",
    "# All losses\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Losses\")\n",
    "plt.plot(epochs_list, loss_per_epoch[\"training\"], label='training')\n",
    "plt.plot(epochs_list, loss_per_epoch[\"validation\"] , label='validation')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "if not flag_debug:\n",
    "    plt.savefig(os.path.join(output_save_directory, current_date + \"_loss\"+\".png\"))\n",
    "\n",
    "# Loss for training\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Loss during training\")\n",
    "plt.plot(reals_shown, [math.log10(y) for y in training_loss_list])\n",
    "plt.xlabel(\"reals_shown\")\n",
    "plt.ylabel(\"Training Log10(Loss)\")\n",
    "if not flag_debug:\n",
    "    plt.savefig(os.path.join(output_save_directory, current_date + \"_training_loss\"+\".png\"))\n",
    "\n",
    "\n",
    "    \n",
    "# Final Model:\n",
    "with torch.no_grad():\n",
    "    input_image = fixed_val_sample['source']\n",
    "    input_images = vutils.make_grid(input_image[0:1,:,:,:], padding=2, normalize=True)\n",
    "    target_images = vutils.make_grid(fixed_val_sample['boneless'][0:1,:,:,:], padding=2, normalize=True)\n",
    "    net = net.cpu()\n",
    "    output_image = net(input_image[0:1,:,:,:]).detach().cpu()\n",
    "    output_images = vutils.make_grid(output_image, padding=2, normalize=True)\n",
    "print(str(torch.max(output_images)) + \",\" + str(torch.min(output_images)))\n",
    "plt.figure(1)\n",
    "fig, ax = plt.subplots(1,3, figsize=(15,15))\n",
    "ax[0].imshow(np.transpose(input_images, (1,2,0)), vmin=0, vmax=1)\n",
    "ax[0].set_title(\"Source\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].imshow(np.transpose(output_images, (1,2,0)), vmin=0, vmax=1)\n",
    "ax[1].set_title(\"Suppressed\")\n",
    "ax[1].axis(\"off\")\n",
    "ax[2].imshow(np.transpose(target_images, (1,2,0)), vmin=0, vmax=1)\n",
    "ax[2].set_title(\"Ideal Bone-suppressed\")\n",
    "ax[2].axis(\"off\")\n",
    "plt.show\n",
    "if not flag_debug:\n",
    "    plt.savefig(os.path.join(output_save_directory, current_date + \"_validation_ComparisonImages\"+\".png\"))\n",
    "\n",
    "# ANIMATED VALIDATION IMAGE\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.axis(\"off\")\n",
    "ims = []\n",
    "training_ims_shown = []\n",
    "for i, im in enumerate(img_list):\n",
    "    if i % 50 == 0:  # controls how many images are printed into the animation\n",
    "        training_ims_shown = i*(100*_batch_size)\n",
    "        frame = ax.imshow(np.transpose(im,(1,2,0)))\n",
    "        t = ax.annotate(\"Reals shown: {}\".format(training_ims_shown), (0.5,1.02), xycoords=\"axes fraction\")\n",
    "        ims.append([frame, t])\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=200, repeat_delay=1000, blit=True)\n",
    "if not flag_debug:\n",
    "    ani.save(os.path.join(output_save_directory, current_date+\"_animation.mp4\"), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c923046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_matrix = torch.from_numpy(np.random.rand(15,1,256,256)).float()\n",
    "true_matrix = torch.from_numpy(np.random.rand(15,1,256,256)).float()\n",
    "print(criterion_Rajaraman(clean_matrix, true_matrix, alpha=0.84))\n",
    "print(len(true_matrix))\n",
    "print(len(ds_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccffbd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_average[\"validation\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
